---
title: 'Practical Machine Learning : Prediction Assignment Writeup'
author: "Johnson Kamireddy"
date: "May 29, 2019"
output:
  pdf_document: default
  html_document:
    keep_md: yes
---

# OVERVIEW:

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of the project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 

This report describe how the model was built, how cross validation was used, what expected out of sample error was, and why/ how the specific choices were made. We used prediction model to predict 20 different test cases.

# Loading and Cleaning data:

Loading the required R libraries to perform the analysis.

```{r, echo=FALSE}

library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(corrplot)

```

The datasets are downloaded into the system so the next step involves loading the datasets into Rstudio. The dataset was partioned into two. 70% of the data is considered to be training set to train the algorithms and remaining 30% as test set to test the trained models.

```{r, echo=TRUE}

training_data <- read.csv(file = "C:/Users/johns/Documents/pml-training.csv", header = TRUE, sep = ",")
test_data <- read.csv(file = "C:/Users/johns/Documents/pml-testing.csv", header = TRUE, sep = ",")

# Creating the data partitions:

train_partition <- createDataPartition(training_data$classe, p = 0.7, list = FALSE)
traindata <- training_data[train_partition, ]
testdata <- training_data[-train_partition, ]

# Dimensions of the training datatset:
dim(traindata)

# Dimensions of the test datatset:
dim(testdata)
```

Both the datasets have 160 variables and there is a possibility that there are plenty NA values present in the dataset.In addition to that the near zero variance variables (NZV) are also removed and the identification variables as well.

```{r, echo=TRUE}

zerovariables <- nearZeroVar(traindata)
traindata <- traindata[, -zerovariables]
testdata <- testdata[, -zerovariables]

dim(traindata)

```
```{r, echo=TRUE}

dim(testdata)

```

Removing the variables that have most NA values
```{r, echo=TRUE}
navariables <- sapply(traindata, function(x) mean(is.na(x))) > 0.95
traindata <- traindata[, navariables == FALSE]
testdata <- testdata[, navariables == FALSE]

dim(traindata)
```

```{r,echo=TRUE}

dim(testdata)

```

Removing the identification variables which are columns 1 to 5

```{r,echo=TRUE}

traindata <- traindata[, -(1:5)]
testdata <- testdata[, -(1:5)]

dim(traindata)

```

```{r,echo=TRUE}

dim(testdata)

```
After the data wrangling we have 54 variables available with 13737 and 5885 records in traindata and testdata datasets.

# Exploratory Data Analysis:

## Correlation Analysis

The correlation analysis is performed on the train dataset before modeling it with machine learning techniques.

```{r, echo=TRUE, fig.height= 15}

cor_matrix <- cor(traindata[, -54])

corrplot(cor_matrix, order = "FPC", method = "color", type = "lower", tl.cex = 0.8, tl.col = rgb(0, 0, 0))

```

# Prediction Modeling:

Three methods are applied to the train dataset to model the regressions and the accurate model with the higher accuracy when applied to test data will be employed for further results or predictions. The methods employed are Random forests, Decision tree and generalized boosted model.

## Random Forests:

```{r,echo=TRUE}

set.seed(12345)
RFcontrol <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
RFmodfit <- train(classe ~ ., data = traindata, method="rf", trControl=RFcontrol)

RFmodfit$finalModel

```

Using the trained model on the test dataset to check the accuracy of the model.

```{r, echo=TRUE}

predict_RF <- predict(RFmodfit, newdata = testdata)
confusion_RF <- confusionMatrix(predict_RF, testdata$classe)
confusion_RF
```

Plotting the results of the consufusion matrix to explain it more visually.

```{r,echo=TRUE}

plot(confusion_RF$table, col = confusion_RF$byclass, main = paste("Random Forest Accuracy = ", round(confusion_RF$overall['Accuracy'], 4)))

```

## Decision Trees:

```{r, echo=TRUE}

set.seed(12345)
DTmodfit <- rpart(classe ~ ., data = traindata, method = "class")
fancyRpartPlot(DTmodfit)

```

Using the trained model on the test dataset to check the accuracy of the model.

```{r, echo=TRUE}

predict_DT <- predict(DTmodfit, newdata = testdata, type = "class")
confusion_DT <- confusionMatrix(predict_DT, testdata$classe)
confusion_DT

```

Plotting the results of the consufusion matrix to explain it more visually.

```{r,echo=TRUE}

plot(confusion_DT$table, col = confusion_DT$byclass, main = paste("Decision Tree Accuracy = ", round(confusion_DT$overall['Accuracy'], 4)))

```

## Generalized Boosted Model:

```{r, echo=TRUE}

set.seed(12345)
GBMcontrol <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
GBMmodfit <- train(classe ~ ., data = traindata, method = "gbm", trControl = GBMcontrol, verbose = FALSE)

GBMmodfit$finalModel

```

```{r,echo=TRUE}

predict_GBM <- predict(GBMmodfit, newdata = testdata)
confusion_GBM <- confusionMatrix(predict_GBM, testdata$classe)
confusion_GBM

```

Plotting the results of the consufusion matrix to explain it more visually.

```{r, echo=TRUE}

plot(confusion_GBM$table, col = confusion_GBM$byClass, main = paste("GBM Accuracy =", round(confusion_GBM$overall['Accuracy'], 4)))

```

# Employing the efficient model to the test data:

The accuracy ratings of the 3 models used are as follows:
I.   Random Forest - 0.99
II.  Decision Tree - 0.73
III. Generalized Boosted Model - 0.98

As the accuracy of Random forest model is more efficient and accurate we will be employing this on the test data set.

```{r, echo=TRUE}

predict_test <- predict(RFmodfit, newdata = test_data)
predict_test

```
